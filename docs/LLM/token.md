---
title: Token
parent: LLMì´ë€ ë¬´ì—‡ì¸ê°€?
nav_order: 3
---

# Token

## ê°œìš”

í† í°(Token)ì€ LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë‹¨ìœ„ì…ë‹ˆë‹¤. ì¸ê°„ì´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, LLMì€ í† í°ì´ë¼ëŠ” ë” ì‘ì€ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í•´í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €(Tokenizer)ëŠ” ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.

## í† í°ì´ë€?

### 1. í† í°ì˜ ì •ì˜

**í† í°(Token)**ì€ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ì˜ ìµœì†Œ ë‹¨ìœ„ì…ë‹ˆë‹¤.

**ì˜ˆì‹œ:**
```
í…ìŠ¤íŠ¸: "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤!"
í† í°: ["ì•ˆë…•", "í•˜ì„¸ìš”", ",", "ë°˜ê°‘", "ìŠµë‹ˆë‹¤", "!"]
```

### 2. í† í°ì˜ íŠ¹ì§•

- **ê³ ì •ëœ ì–´íœ˜ì§‘**: ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ í† í°ë“¤ì˜ ì§‘í•©
- **ìˆ«ì ì¸ë±ìŠ¤**: ê° í† í°ì€ ê³ ìœ í•œ ìˆ«ìë¡œ ë§¤í•‘
- **ë²¡í„° ë³€í™˜**: í† í°ì€ ì„ë² ë”©ì„ í†µí•´ ë²¡í„°ë¡œ ë³€í™˜

### 3. í† í°í™”ì˜ í•„ìš”ì„±

**ì™œ í† í°í™”ê°€ í•„ìš”í•œê°€?**

1. **ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¤‘êµ­ì–´ ë“± ëª¨ë“  ì–¸ì–´ë¥¼ í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
2. **ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ ì²˜ë¦¬**: í•™ìŠµ ì‹œ ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ë‹¨ì–´ë„ ì²˜ë¦¬ ê°€ëŠ¥
3. **íš¨ìœ¨ì ì¸ ì²˜ë¦¬**: í…ìŠ¤íŠ¸ë¥¼ ì¼ì •í•œ í¬ê¸°ì˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬

## í† í¬ë‚˜ì´ì €ì˜ ì¢…ë¥˜

### 1. ë‹¨ì–´ ê¸°ë°˜ í† í¬ë‚˜ì´ì € (Word-based Tokenizer)

**íŠ¹ì§•:**
- ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬
- ê°€ì¥ ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- ì–´íœ˜ì§‘ í¬ê¸°ê°€ ë§¤ìš° í´ ìˆ˜ ìˆìŒ

**ì˜ˆì‹œ:**
```python
# ì˜ì–´ ì˜ˆì‹œ
text = "I love artificial intelligence"
tokens = ["I", "love", "artificial", "intelligence"]

# í•œêµ­ì–´ ì˜ˆì‹œ (ê³µë°± ê¸°ì¤€)
text = "ë‚˜ëŠ” ì¸ê³µì§€ëŠ¥ì„ ì¢‹ì•„í•©ë‹ˆë‹¤"
tokens = ["ë‚˜ëŠ”", "ì¸ê³µì§€ëŠ¥ì„", "ì¢‹ì•„í•©ë‹ˆë‹¤"]
```

**ì¥ë‹¨ì :**
- **ì¥ì **: ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- **ë‹¨ì **: ì–´íœ˜ì§‘ì´ ë§¤ìš° í¬ê³ , ìƒˆë¡œìš´ ë‹¨ì–´ ì²˜ë¦¬ ì–´ë ¤ì›€

### 2. ë¬¸ì ê¸°ë°˜ í† í¬ë‚˜ì´ì € (Character-based Tokenizer)

**íŠ¹ì§•:**
- ê° ë¬¸ìë¥¼ ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬
- ì–´íœ˜ì§‘ í¬ê¸°ê°€ ë§¤ìš° ì‘ìŒ
- ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ

**ì˜ˆì‹œ:**
```python
text = "Hello"
tokens = ["H", "e", "l", "l", "o"]

text = "ì•ˆë…•í•˜ì„¸ìš”"
tokens = ["ì•ˆ", "ë…•", "í•˜", "ì„¸", "ìš”"]
```

**ì¥ë‹¨ì :**
- **ì¥ì **: ì–´íœ˜ì§‘ì´ ì‘ê³ , ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥
- **ë‹¨ì **: í† í° ìˆ˜ê°€ ë§ì•„ì§€ê³ , ì˜ë¯¸ ì •ë³´ ì†ì‹¤

### 3. ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € (Subword Tokenizer)

**íŠ¹ì§•:**
- ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í•´
- ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´ì„ í•™ìŠµ
- ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë„ ì²˜ë¦¬ ê°€ëŠ¥

**ëŒ€í‘œì ì¸ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €:**

#### BPE (Byte Pair Encoding)
```python
# BPE ì˜ˆì‹œ
text = "artificial intelligence"
# í•™ìŠµ ê³¼ì •ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ì„ ì°¾ì•„ í† í°í™”
tokens = ["art", "ificial", "intel", "ligence"]
```

#### WordPiece
```python
# WordPiece ì˜ˆì‹œ (BERTì—ì„œ ì‚¬ìš©)
text = "artificial intelligence"
tokens = ["art", "##ificial", "intel", "##ligence"]
# ##ì€ ì„œë¸Œì›Œë“œì„ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œì‹œ
```

#### SentencePiece
```python
# SentencePiece ì˜ˆì‹œ (ë‹¤êµ­ì–´ ì§€ì›)
text = "ì•ˆë…•í•˜ì„¸ìš” Hello"
tokens = ["â–ì•ˆë…•", "í•˜ì„¸ìš”", "â–Hello"]
# â–ì€ ë‹¨ì–´ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œì‹œ
```

## ì£¼ìš” í† í¬ë‚˜ì´ì € ë¹„êµ

### 1. GPT ê³„ì—´ (BPE)
**íŠ¹ì§•:**
- ì˜ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ê³„
- ì›¹ í¬ë¡¤ë§ ë°ì´í„°ë¡œ í•™ìŠµ
- ëŒ€ì†Œë¬¸ì êµ¬ë¶„

**ì˜ˆì‹œ:**
```python
# GPT í† í¬ë‚˜ì´ì €
text = "Hello, world!"
tokens = ["Hello", ",", "Ä world", "!"]
# Ä ëŠ” ê³µë°± ë‹¤ìŒì— ì˜¤ëŠ” í† í°ì„ ë‚˜íƒ€ëƒ„
```

### 2. BERT ê³„ì—´ (WordPiece)
**íŠ¹ì§•:**
- ì˜ì–´ ì¤‘ì‹¬ì´ì§€ë§Œ ë‹¤êµ­ì–´ ì§€ì›
- ëŒ€ì†Œë¬¸ì êµ¬ë¶„
- ì„œë¸Œì›Œë“œ í‘œì‹œë¡œ ## ì‚¬ìš©

**ì˜ˆì‹œ:**
```python
# BERT í† í¬ë‚˜ì´ì €
text = "artificial intelligence"
tokens = ["art", "##ificial", "intel", "##ligence"]
```

### 3. T5 ê³„ì—´ (SentencePiece)
**íŠ¹ì§•:**
- ë‹¤êµ­ì–´ ì§€ì›
- ì–¸ì–´ êµ¬ë¶„ ì—†ì´ í†µí•© ì²˜ë¦¬
- ë‹¨ì–´ ì‹œì‘ í‘œì‹œë¡œ â– ì‚¬ìš©

**ì˜ˆì‹œ:**
```python
# T5 í† í¬ë‚˜ì´ì €
text = "ì•ˆë…•í•˜ì„¸ìš” Hello"
tokens = ["â–ì•ˆë…•", "í•˜ì„¸ìš”", "â–Hello"]
```

## í† í¬ë‚˜ì´ì €ì˜ ì‘ë™ ì›ë¦¬

### 1. í•™ìŠµ ê³¼ì •

**BPE í•™ìŠµ ì˜ˆì‹œ:**
```python
# 1ë‹¨ê³„: ì´ˆê¸° ì–´íœ˜ì§‘ ìƒì„±
vocab = {"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"}

# 2ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬
text = "artificial intelligence"
chars = ["a", "r", "t", "i", "f", "i", "c", "i", "a", "l", " ", "i", "n", "t", "e", "l", "l", "i", "g", "e", "n", "c", "e"]

# 3ë‹¨ê³„: ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ìŒì„ ì°¾ì•„ ë³‘í•©
# "ar" + "t" = "art"
# "intel" + "ligence" = "intelligence"

# 4ë‹¨ê³„: ìµœì¢… í† í°
tokens = ["art", "ificial", "intelligence"]
```

### 2. í† í°í™” ê³¼ì •

```python
def tokenize_text(text, tokenizer):
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text = preprocess(text)
    
    # 2. í† í°í™”
    tokens = tokenizer.encode(text)
    
    # 3. í† í° IDë¡œ ë³€í™˜
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    return token_ids

# ì˜ˆì‹œ
text = "ì•ˆë…•í•˜ì„¸ìš”"
token_ids = [101, 102, 103, 104, 105]  # ì‹¤ì œ IDëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
```

### 3. ì—­í† í°í™” ê³¼ì •

```python
def detokenize_text(token_ids, tokenizer):
    # 1. IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
    tokens = tokenizer.convert_ids_to_tokens(token_ids)
    
    # 2. í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    text = tokenizer.convert_tokens_to_string(tokens)
    
    return text

# ì˜ˆì‹œ
token_ids = [101, 102, 103, 104, 105]
text = "ì•ˆë…•í•˜ì„¸ìš”"
```

## í† í°í™”ì˜ ì‹¤ì œ ì˜í–¥

### 1. í† í° ìˆ˜ì™€ ë¹„ìš©

**í† í° ìˆ˜ ê³„ì‚°:**
```python
def count_tokens(text, tokenizer):
    tokens = tokenizer.encode(text)
    return len(tokens)

# ì˜ˆì‹œ
text = "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤!"
token_count = count_tokens(text, tokenizer)  # ì˜ˆ: 8ê°œ í† í°
```

**ë¹„ìš© ê³„ì‚°:**
```python
def calculate_cost(token_count, price_per_1k_tokens):
    cost = (token_count / 1000) * price_per_1k_tokens
    return cost

# ì˜ˆì‹œ
token_count = 1000
price_per_1k = 0.002  # $0.002 per 1K tokens
cost = calculate_cost(token_count, price_per_1k)  # $0.002
```

### 2. ì–¸ì–´ë³„ í† í° íš¨ìœ¨ì„±

**ì˜ì–´ vs í•œêµ­ì–´ ë¹„êµ:**
```python
# ì˜ì–´
english_text = "Hello, how are you?"
english_tokens = ["Hello", ",", "Ä how", "Ä are", "Ä you", "?"]  # 6ê°œ í† í°

# í•œêµ­ì–´
korean_text = "ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"
korean_tokens = ["ì•ˆë…•", "í•˜ì„¸ìš”", ",", "ì–´ë–»ê²Œ", "ì§€ë‚´ì„¸ìš”", "?"]  # 6ê°œ í† í°

# ê°™ì€ ì˜ë¯¸ì§€ë§Œ í† í° ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
```

### 3. í† í° ì œí•œê³¼ ì²˜ë¦¬

**ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ:**
```python
def check_context_length(text, tokenizer, max_tokens=4096):
    tokens = tokenizer.encode(text)
    token_count = len(tokens)
    
    if token_count > max_tokens:
        # í† í° ìˆ˜ê°€ ì œí•œì„ ì´ˆê³¼í•˜ë©´ ì˜ë¼ë‚´ê¸°
        tokens = tokens[:max_tokens]
        text = tokenizer.decode(tokens)
        return text, token_count
    else:
        return text, token_count

# ì˜ˆì‹œ
long_text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸..."
truncated_text, count = check_context_length(long_text, tokenizer)
```

## í† í° ìˆ˜ ê³„ì‚°ê³¼ ë¶„ì„

### 1. í† í° ìˆ˜ ê³„ì‚° ë°©ë²•

**ê¸°ë³¸ í† í° ìˆ˜ ê³„ì‚°:**
```python
def count_tokens(text, tokenizer):
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    tokens = tokenizer.encode(text)
    return len(tokens)

def count_tokens_detailed(text, tokenizer):
    """ìƒì„¸í•œ í† í° ì •ë³´ ë°˜í™˜"""
    tokens = tokenizer.encode(text)
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    return {
        "text": text,
        "tokens": tokens,
        "token_ids": token_ids,
        "token_count": len(tokens),
        "character_count": len(text),
        "tokens_per_character": len(tokens) / len(text) if text else 0
    }

# ì˜ˆì‹œ
text = "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤!"
result = count_tokens_detailed(text, tokenizer)
print(f"í† í° ìˆ˜: {result['token_count']}")
print(f"ë¬¸ì ìˆ˜: {result['character_count']}")
print(f"ë¬¸ìë‹¹ í† í° ìˆ˜: {result['tokens_per_character']:.2f}")
```

### 2. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìœ í˜•ë³„ í† í° ìˆ˜ ë¶„ì„

```python
def analyze_token_usage_by_type():
    """í…ìŠ¤íŠ¸ ìœ í˜•ë³„ í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„"""
    
    sample_texts = {
        "ì˜ì–´": "Hello, how are you today?",
        "í•œêµ­ì–´": "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "ì½”ë“œ": "def hello_world(): print('Hello, World!')",
        "ìˆ«ì": "1234567890",
        "íŠ¹ìˆ˜ë¬¸ì": "!@#$%^&*()",
        "ì´ëª¨ì§€": "ì•ˆë…•í•˜ì„¸ìš” ğŸ˜Š ë°˜ê°‘ìŠµë‹ˆë‹¤ ğŸ‘‹"
    }
    
    results = {}
    for text_type, text in sample_texts.items():
        tokens = tokenizer.encode(text)
        results[text_type] = {
            "text": text,
            "token_count": len(tokens),
            "character_count": len(text),
            "efficiency": len(text) / len(tokens) if tokens else 0
        }
    
    return results

# ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ
analysis = analyze_token_usage_by_type()
for text_type, data in analysis.items():
    print(f"{text_type}: {data['token_count']} í† í° ({data['character_count']} ë¬¸ì)")
```

### 3. í† í° ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸

```python
def predict_token_count(text, language="korean"):
    """ì–¸ì–´ë³„ í† í° ìˆ˜ ì˜ˆì¸¡"""
    
    # ì–¸ì–´ë³„ í‰ê·  í† í° ë¹„ìœ¨
    token_ratios = {
        "korean": 0.8,    # í•œêµ­ì–´: ë¬¸ìë‹¹ ì•½ 0.8 í† í°
        "english": 0.4,   # ì˜ì–´: ë¬¸ìë‹¹ ì•½ 0.4 í† í°
        "chinese": 1.2,   # ì¤‘êµ­ì–´: ë¬¸ìë‹¹ ì•½ 1.2 í† í°
        "code": 0.6,      # ì½”ë“œ: ë¬¸ìë‹¹ ì•½ 0.6 í† í°
        "mixed": 0.7      # í˜¼í•©: ë¬¸ìë‹¹ ì•½ 0.7 í† í°
    }
    
    estimated_tokens = int(len(text) * token_ratios.get(language, 0.7))
    return estimated_tokens

def estimate_cost_by_tokens(token_count, model="gpt-3.5-turbo"):
    """í† í° ìˆ˜ì— ë”°ë¥¸ ë¹„ìš© ì¶”ì •"""
    
    # ëª¨ë¸ë³„ í† í°ë‹¹ ë¹„ìš© (USD)
    costs_per_1k_tokens = {
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-4-turbo": {"input": 0.01, "output": 0.03}
    }
    
    model_costs = costs_per_1k_tokens.get(model, {"input": 0.0015, "output": 0.002})
    
    # ì…ë ¥ í† í° ë¹„ìš© (ì˜ˆìƒ)
    input_cost = (token_count * model_costs["input"]) / 1000
    
    # ì¶œë ¥ í† í° ë¹„ìš© (ì˜ˆìƒ - ì…ë ¥ì˜ 50%ë¡œ ê°€ì •)
    output_cost = (token_count * 0.5 * model_costs["output"]) / 1000
    
    total_cost = input_cost + output_cost
    
    return {
        "input_tokens": token_count,
        "estimated_output_tokens": int(token_count * 0.5),
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

# ì‚¬ìš© ì˜ˆì‹œ
text = "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
estimated_tokens = predict_token_count(text, "korean")
cost_estimate = estimate_cost_by_tokens(estimated_tokens, "gpt-3.5-turbo")

print(f"ì˜ˆìƒ í† í° ìˆ˜: {estimated_tokens}")
print(f"ì˜ˆìƒ ë¹„ìš©: ${cost_estimate['total_cost']:.4f}")
```

### 4. í† í° ìˆ˜ ìµœì í™” ë„êµ¬

```python
class TokenOptimizer:
    def __init__(self, tokenizer, target_tokens=1000):
        self.tokenizer = tokenizer
        self.target_tokens = target_tokens
    
    def optimize_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ëª©í‘œ í† í° ìˆ˜ì— ë§ê²Œ ìµœì í™”"""
        current_tokens = len(self.tokenizer.encode(text))
        
        if current_tokens <= self.target_tokens:
            return text, current_tokens
        
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        optimized = re.sub(r'\s+', ' ', text).strip()
        
        # 2. ë°˜ë³µë˜ëŠ” í‘œí˜„ ì œê±°
        optimized = self._remove_redundant_expressions(optimized)
        
        # 3. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        if len(self.tokenizer.encode(optimized)) > self.target_tokens:
            optimized = self._truncate_by_sentences(optimized)
        
        final_tokens = len(self.tokenizer.encode(optimized))
        return optimized, final_tokens
    
    def _remove_redundant_expressions(self, text):
        """ë°˜ë³µë˜ëŠ” í‘œí˜„ ì œê±°"""
        # ê°„ë‹¨í•œ ì˜ˆì‹œ: ì—°ì†ëœ ë™ì¼í•œ ë‹¨ì–´ ì œê±°
        words = text.split()
        cleaned_words = []
        prev_word = None
        
        for word in words:
            if word != prev_word:
                cleaned_words.append(word)
            prev_word = word
        
        return ' '.join(cleaned_words)
    
    def _truncate_by_sentences(self, text):
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°"""
        sentences = text.split('.')
        result = ""
        
        for sentence in sentences:
            test_text = result + sentence + "."
            if len(self.tokenizer.encode(test_text)) <= self.target_tokens:
                result = test_text
            else:
                break
        
        return result.strip()

# ì‚¬ìš© ì˜ˆì‹œ
optimizer = TokenOptimizer(tokenizer, target_tokens=100)
long_text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ë‚´ìš©..."
optimized_text, token_count = optimizer.optimize_text(long_text)
print(f"ìµœì í™”ëœ í† í° ìˆ˜: {token_count}")
```

### 5. ë°°ì¹˜ í† í° ìˆ˜ ê³„ì‚°

```python
def calculate_batch_tokens(texts, tokenizer):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ë°°ì¹˜ë¡œ ê³„ì‚°"""
    results = []
    total_tokens = 0
    
    for i, text in enumerate(texts):
        tokens = tokenizer.encode(text)
        token_count = len(tokens)
        total_tokens += token_count
        
        results.append({
            "index": i,
            "text": text[:50] + "..." if len(text) > 50 else text,
            "token_count": token_count,
            "cumulative_tokens": total_tokens
        })
    
    return {
        "individual_results": results,
        "total_tokens": total_tokens,
        "average_tokens": total_tokens / len(texts) if texts else 0
    }

# ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì‹œ
texts = [
    "ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
    "ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
    "ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
]

batch_result = calculate_batch_tokens(texts, tokenizer)
print(f"ì´ í† í° ìˆ˜: {batch_result['total_tokens']}")
print(f"í‰ê·  í† í° ìˆ˜: {batch_result['average_tokens']:.1f}")
```

## ì‹¤ë¬´ì—ì„œì˜ ê³ ë ¤ì‚¬í•­

### 1. í† í¬ë‚˜ì´ì € ì„ íƒ

**ì–¸ì–´ë³„ ê¶Œì¥ í† í¬ë‚˜ì´ì €:**
```python
tokenizer_recommendations = {
    "ì˜ì–´": "GPT, BERT í† í¬ë‚˜ì´ì €",
    "í•œêµ­ì–´": "KoBERT, KoGPT í† í¬ë‚˜ì´ì €", 
    "ë‹¤êµ­ì–´": "mT5, XLM-R í† í¬ë‚˜ì´ì €",
    "ì½”ë“œ": "CodeGPT, CodeBERT í† í¬ë‚˜ì´ì €"
}
```

### 2. í† í° íš¨ìœ¨ì„± ìµœì í™”

```python
def optimize_token_usage(text, tokenizer):
    # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = text.strip()
    
    # 2. ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±°
    text = remove_redundant_patterns(text)
    
    # 3. í† í° ìˆ˜ í™•ì¸
    token_count = len(tokenizer.encode(text))
    
    return text, token_count

# ì˜ˆì‹œ
original_text = "   ì•ˆë…•í•˜ì„¸ìš”   ë°˜ê°‘ìŠµë‹ˆë‹¤   "
optimized_text, count = optimize_token_usage(original_text, tokenizer)
```

### 3. í† í° ì˜ˆì‚° ê´€ë¦¬

```python
class TokenBudget:
    def __init__(self, max_tokens=4096):
        self.max_tokens = max_tokens
        self.used_tokens = 0
    
    def add_tokens(self, token_count):
        self.used_tokens += token_count
        return self.used_tokens <= self.max_tokens
    
    def get_remaining_tokens(self):
        return max(0, self.max_tokens - self.used_tokens)

# ì‚¬ìš© ì˜ˆì‹œ
budget = TokenBudget(max_tokens=4096)
text = "ì•ˆë…•í•˜ì„¸ìš”"
tokens = tokenizer.encode(text)

if budget.add_tokens(len(tokens)):
    print(f"í† í° ì¶”ê°€ ì„±ê³µ. ë‚¨ì€ í† í°: {budget.get_remaining_tokens()}")
else:
    print("í† í° ì œí•œ ì´ˆê³¼!")
```

## ê²°ë¡ 

í† í°ê³¼ í† í¬ë‚˜ì´ì €ëŠ” LLMì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì ì ˆí•œ í† í¬ë‚˜ì´ì € ì„ íƒê³¼ í† í° íš¨ìœ¨ì„± ê´€ë¦¬ëŠ” LLM ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ê³¼ ë¹„ìš©ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **í† í°ì˜ ì¤‘ìš”ì„±**: LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„
2. **í† í¬ë‚˜ì´ì €ì˜ ì¢…ë¥˜**: ë‹¨ì–´, ë¬¸ì, ì„œë¸Œì›Œë“œ ê¸°ë°˜ í† í¬ë‚˜ì´ì €
3. **ì–¸ì–´ë³„ íŠ¹ì„±**: ì˜ì–´, í•œêµ­ì–´ ë“± ì–¸ì–´ì— ë”°ë¥¸ í† í¬ë‚˜ì´ì € ì°¨ì´
4. **ì‹¤ë¬´ ê³ ë ¤ì‚¬í•­**: í† í° ìˆ˜ ì œí•œ, ë¹„ìš© ê´€ë¦¬, íš¨ìœ¨ì„± ìµœì í™”

ì´ëŸ¬í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•  ë•Œ ì ì ˆí•œ í† í¬ë‚˜ì´ì €ë¥¼ ì„ íƒí•˜ê³  í† í° ì‚¬ìš©ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
