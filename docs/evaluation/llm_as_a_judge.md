---
layout: page
title: LLM as a Judge
parent: Evaluation
nav_order: 5
---

# LLM as a Judge

## 개념

LLM as a Judge는 하나의 LLM을 "평가자"로 사용해 다른 모델의 출력(텍스트, 코드, 요약 등)을 채점·비교·심사하는 방법론입니다.

사람 평가의 비용·확장성 문제와 전통 자동지표(예: BLEU/ROUGE)의 표현력 한계를 보완하려는 실용적 대안으로 등장했습니다.

## 최초의 도입과 이유

2023년 공개된 "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"가 이 프레이밍을 본격적으로 정립하고 벤치마크/운영체계를 제안한 대표 사례로 널리 인용됩니다.

### 도입 이유

- **인간 평가의 확장성 부족**: 대규모 실험·A/B 비교를 사람만으로 커버하기 어려움
- **기존 자동지표의 표현력 한계**: 개방형 질의에서 유용성, 논리성, 맥락 충실도 등을 충분히 포착하지 못함
- **비용·속도·일관성**: LLM을 평가자로 쓰면 대규모 반복 평가와 신속한 모델 선택·튜닝이 가능

## 신뢰도

LLM 판사의 판단이 인간 평가와 높은 상관을 보인다는 결과가 보고되었으나, 과제·프롬프트·모델 편향에 따라 일관도가 달라질 수 있습니다. 즉, "사람 수준" 일반화로 단정하기보다는 조건부로 해석해야 합니다.

## 현재 활용 현황

모델 개발 전주기에서 표준화된 프롬프트와 스키마를 갖춘 "자동 평가 파이프라인"으로 활용됩니다.

### 대표적 활용 시나리오

- **챗봇/비서 응답 품질 평가**: 관련성, 정확성, 톤, 유용성 등 다기준 채점
- **RAG 평가**: 출처 충실도(groundedness), 환각 여부, 인용의 적절성 심사
- **생성 품질 관리**: 요약/번역/코드 생성 결과의 요구사항 충족 여부 점검
- **안전성 심사**: 유해·편향·정책 위반 출력에 대한 필터링과 레드팀 강화
- **에이전트 평가**: 계획·도구사용·중간 추론의 타당성 검토

### 평가 형태

- **단일 출력 채점(참조 없음)**: 입력+출력만으로 스코어/설명 생성
- **단일 출력 채점(참조 있음)**: 기준 답안·가이드라인을 함께 제시해 일관성 강화
- **쌍대 비교(pairwise)**: 두 출력 중 더 나은 쪽을 선택(아레나식 ELO 등과 결합)

## 실무 적용 팁

### 프롬프트 설계

- 역할·기준·가중치·출력 스키마를 명시하고, 채점과 근거 설명을 분리
- length bias, verbosity bias를 줄이기 위해 "간결성"과 "근거 인용"을 별도 채점

### 레퍼런스 정렬

- RAG나 정답이 있는 태스크에서는 근거 문서/골드 레이블을 제공해 판정 변동성 축소

### 메타-평가

- 소량의 인간 평가 골드셋을 유지해 LLM 판정의 드리프트를 주기적으로 점검
- 프롬프트 민감도 테스트(perturbation)와 블라인드 재채점으로 견고성 확인

### 모델 선택

- 판사와 피평가 모델의 크로스 모델/버전 편향을 완화하기 위해 다수 판사 앙상블(예: 서로 다른 LLM 2~3개) 또는 다중 프롬프트를 사용

### 출력 형식

- 스코어(예: 1–10) + 이유 + 근거 인용(있는 경우)을 구조화(JSON)하여 자동 집계/모니터링에 활용

## 한계와 주의점

- **편향 전이**: 판사 LLM의 스타일·버전 편향이 순위에 영향을 줄 수 있음
- **프롬프트 의존성**: 지시어의 사소한 변경으로 점수가 흔들릴 수 있음
- **자기평가 리스크**: 동일 계열 모델이 판사와 출력을 동시에 맡을 때 유리하게 치우칠 수 있음. 가급적 이종 모델/앙상블 사용
- **해석 가능성**: 고득점이 실제 사용자 효용과 항상 일치하지는 않음. 사용자 연구와 병행 필요

## 요약

LLM as a Judge는 대규모 인간 평가의 병목과 전통 지표의 한계를 보완하는 실전 중심 평가 패턴입니다.

MT-Bench와 Chatbot Arena가 이 접근을 체계화한 대표 연구로 자리 잡았고, 현재는 자동화된 품질·안전·랭킹 평가의 사실상 표준으로 활용됩니다.

효과를 극대화하려면 레퍼런스 제공, 메타-평가, 앙상블 판정, 구조화 출력 등 운영상의 안전장치를 함께 도입하는 것이 권장됩니다.

## 참고 자료

- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)
- [MT-Bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)
- [Chatbot Arena](https://chat.lmsys.org/)
