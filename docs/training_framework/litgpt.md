---
title: LitGPT
parent: LLM 학습 프레임워크
nav_order: 3
---

# LitGPT

## 개요
LitGPT는 Lightning AI가 공개한 오픈소스 LLM 개발·학습 툴킷으로, 20개+ 고성능 LLM에 대해 사전학습(pretraining), 미세조정(finetuning), 평가, 추론, 배포까지 한 번에 다룰 수 있도록 설계된 "스크립트 중심·해커블(hackable)" 플랫폼이다. Apache-2.0 라이선스 기반으로 모델 구현을 처음부터 최적화해 성능과 비용 효율을 높였고, Hugging Face 모델 허브 연동, 분산 학습, 다양한 정밀도/메모리 최적화, LoRA/QLoRA 등 최신 기법을 폭넓게 지원한다.

## 핵심 철학과 아키텍처
- **최소·모듈형 설계**: "간결하지만 뜯어고치기 쉬운" 구조로, 스크립트를 가져다 바로 수정하며 실험하기 좋은 형태를 지향한다.
- **일관된 CLI 워크플로우**: `litgpt pretrain`, `finetune`, `evaluate` 등 명령 기반으로 동일한 패턴의 실행을 제공한다.
- **전체 스택 최적화**: 모델 구현, 데이터 파이프라인(litdata), 분산 실행(FSDP, multi-node), 정밀도/메모리 최적화까지 엔드투엔드로 고려한다.

## 지원 범위와 모델
- **20개+ LLM 레시피**: Llama 계열, TinyLlama, Gemma 등 다수 모델을 커버하며, 모델별 고성능 레시피를 제공한다.
- **모델 구현을 "처음부터" 최적화**: 속도·비용 최적화 목표로 파이썬/파이토치 레벨에서 직접 구현된 경량·고성능 모델을 제공한다.

## 학습(사전학습/미세조정) 기능

### 사전학습(Pretraining)
- **분산/대규모 설정 기본값**: FSDP, bfloat16, gradient accumulation 등 대규모 학습 표준 구성을 기본 제공하며, 예시 구성에서 최소 8×A100을 권장한다.
- **구성 유연성**: 모델명, 사이즈, 데이터셋, 하이퍼파라미터를 CLI 인자로 손쉽게 변경 가능하며, 멀티노드 실행(플랫폼/SLURM/베어메탈/MPI) 가이드를 제공한다.
- **체크포인트 관리와 재시작**: 단계별 저장·resume, 최종 변환(convert)으로 추론/미세조정에 바로 활용 가능한 체크포인트 포맷을 지원한다.
- **실증 튜토리얼**: TinyLlama 1.1B 사전학습 전체 플로우(데이터 전처리→사전학습→체크포인트 변환)를 자세히 안내한다.

### 미세조정(Finetuning)
- **LoRA/QLoRA/어댑터**: 파라미터 효율형 기법을 폭넓게 지원하여 GPU 메모리 요구량을 낮추고 효율적인 도메인 적응을 가능하게 한다.
- **사용자 데이터 연동**: 커스텀 데이터셋 문서화와 파이프라인 제공에 집중하고 있으며, 실무 이슈(다중 GPU, CUDA 호환 등)에 대한 개선이 활발히 진행 중이다.

## 데이터 파이프라인과 최적화

- **litdata 통합**: 대용량 텍스트를 바이너리 청크로 변환하는 스트리밍 최적화 파이프라인을 제공하여 IO 병목을 줄이고 학습 속도를 높인다.
- **대규모 데이터 전처리 스크립트**: SlimPajama, StarCoder 등 공개 데이터셋의 준비 스크립트를 제공하며, 필요한 디스크 용량과 실행 예시를 명확히 제시한다(예: SlimPajama≈2.5TB, StarCoder≈1.1TB).
- **빠른 개발 실행**: 데이터 준비·학습 단계에서 `--fast_dev_run` 플래그로 소량 샘플을 빠르게 시험해볼 수 있다.

## 분산 학습과 자원 요구사항

- **분산 실행 백엔드**: FSDP 기반의 대규모 분산 사전학습을 지원하며, Lightning AI 플랫폼·SLURM·MPI 등 다양한 환경 가이드가 포함된다.
- **하드웨어 요구 리소스 표**: 모델별/작업별 VRAM 요구 및 팁을 정리한 리소스 표 자료를 제공한다.
- **실제 요구 사례**: TinyLlama 사전학습 기본 설정에서 8×A100 권장, 단일 머신에서는 수 주 소요될 수 있으며 클러스터 사용을 권장한다.

## 추론·평가·배포

- **체크포인트 변환과 재사용**: 학습 산출물을 평가·추론·미세조정에 바로 사용할 수 있도록 변환 툴을 제공한다.
- **추론/배포 경량화**: 정밀도 전환, 양자화(예: int8/4) 등 메모리 최적화 기법을 활용해 소비자급 GPU에서도 실행 가능하도록 돕는다.
- **Lightning 플랫폼 연계**: 브라우저 기반 스튜디오/워크스페이스에서 멀티 GPU·멀티 노드까지 제로-셋업으로 실행 가능한 프로젝트 템플릿과 빠른 시작 환경을 제공한다.

## 사용성: 튜토리얼·템플릿·CLI

- **튜토리얼**: TinyLlama 사전학습, 데이터 준비, OOM 대응, 로깅(W&B) 통합 등 실무형 가이드가 단계별로 제공된다.
- **스튜디오 템플릿**: TinyLlama 1T 토큰 데이터 준비, Pretrain LLMs - TinyLlama 1.1B, Continued Pretraining 등의 재현 가능한 프로젝트 템플릿을 제공한다.
- **CLI/패키지**: `litgpt` 명령 기반으로 사전학습/미세조정/평가/변환을 수행하며, PyPI 배포 패키지도 제공한다.

## 커뮤니티·활동성·프로젝트 건강도

- **활발한 개발**: CUDA/멀티GPU 이슈 대응, 문서 강화, LoRA/양자화 가이드 등의 개선이 이어지고 있으며, 핵심 기여자군이 적극적으로 유지보수 중이다.
- **프로젝트 포지셔닝**: Lit-LLaMA의 후속/대체 격으로 LitGPT가 안내되며, 최신 기능과 레시피는 LitGPT에 집중되고 있다.
- **커뮤니티 반응**: GitHub 트렌딩 등 주목도를 보인 바 있으며, 스타·포크·이슈 활동이 활발하다.

## 실전 예시: TinyLlama 사전학습 워크플로우 요약

1. **토크나이저 준비 및 의존성 설치**  
   - `litgpt download`로 HF 토크나이저 구성 다운로드 후, `pip install .[all]`로 전처리 의존성 설치.

2. **데이터 전처리(바이너리 청크화)**  
   - StarCoder, SlimPajama용 스크립트로 `input_dir`→`output_dir` 변환 실행(대용량 디스크 필요).

3. **사전학습 실행**  
   - `litgpt pretrain --config config_hub/pretrain/tinyllama.yaml`  
   - 기본 FSDP+bfloat16+gradient accumulation, 8×A100 권장.

4. **체크포인트 재시작/변환**  
   - `--resume`로 중단점 재개, `convert_pretrained_checkpoint`로 최종 산출물 정리 후 추론·평가·미세조정에 활용.

5. **클러스터 실행/로깅**  
   - Lightning/SLURM/MPI 가이드 제공, W&B 연동(`--logger_name=wandb`).

## LitGPT를 선택할 이유와 한계

### 장점
- 단일 도구로 사전학습→미세조정→평가→배포까지 통합.
- "스크립트 우선" 접근으로 연구·엔지니어링 커스텀에 유리하며, 깔끔한 CLI·템플릿 제공.
- 대용량 데이터 스트리밍 최적화(litdata), 분산 학습(FSDP), 다양한 효율화(LoRA/QLoRA/정밀도/양자화) 지원.

### 유의점
- 대규모 사전학습은 여전히 고성능/다중 GPU 자원이 필요하고, 단일 머신은 장시간 소요.
- 멀티GPU·CUDA 환경 다양성으로 인한 호환 이슈가 보고되며 문서/가이드 보강이 진행 중.

## 결론
LitGPT는 LLM 연구·서비스 개발 양면을 모두 고려한 "현실적인" 학습 플랫폼으로, 재현성 높은 레시피, 강력한 데이터/분산/최적화 스택, 그리고 커스터마이즈 용이한 스크립트 기반 워크플로우를 통해 사전학습과 미세조정을 빠르게 산업/연구 현장에 적용하기 좋다. Lightning AI 스튜디오/플랫폼과 결합하면 제로-셋업 멀티노드 실험과 배포까지 끌고 갈 수 있어, LLM 서비스 개발 초기부터 운영까지 지속적으로 확장 가능한 기반을 제공한다.

- **공식 저장소·문서**: 기능/레시피/튜토리얼 일체 제공.
- **빠른 시작**: Lightning 스튜디오 템플릿 기반 실험 가속.
- **대용량 데이터·분산 학습·효율화 기술까지 일관된 개발 경험 제공**.
