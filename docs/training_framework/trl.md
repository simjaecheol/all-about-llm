---
title: TRL
parent: LLM í•™ìŠµ í”„ë ˆì„ì›Œí¬
nav_order: 5
---

# TRL(Transformer Reinforcement Learning) í•™ìŠµ í”Œë«í¼ ì™„ì „ ë¶„ì„

## 1. ê°œìš”  
TRLì€ Hugging Faceê°€ ì œê³µí•˜ëŠ” **Transformer ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì˜ ê°•í™”í•™ìŠµ(RLHF) ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¡œ, ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì„ Proximal Policy Optimization(PPO), Direct Preference Optimization(DPO), Group Relative Policy Optimization(GRPO) ë“± ë‹¤ì–‘í•œ RL ê¸°ë²•ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ë˜í•œ Supervised Fine-Tuning(SFT)ê³¼ Reward Modeling(RM)ì„ í¬í•¨í•œ í’€ìŠ¤íƒ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•˜ë©°, ğŸ¤— Transformersì™€ ì™„ì „ í†µí•©ë˜ì–´ ê°„í¸í•œ ì‹¤í—˜ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

## 2. í•µì‹¬ íŠ¹ì§•  

### 2.1 í†µí•© Trainer API  
- **SFTTrainer**: ì§€ë„í•™ìŠµ ê¸°ë°˜ ë¯¸ì„¸ì¡°ì • ì§€ì›  
- **RewardTrainer**: ì¸ê°„ í”¼ë“œë°± ê¸°ë°˜ ë³´ìƒëª¨ë¸ í•™ìŠµ ì§€ì›  
- **PPOTrainer**: (query, response, reward) ì‚¼ì¤‘í•­ë§Œìœ¼ë¡œ PPO ê°•í™”í•™ìŠµ ìˆ˜í–‰  
- **DPOTrainer**: ì§ì ‘ ì„ í˜¸ë„ ìµœì í™”(Direct Preference Optimization)  
- **GRPOTrainer**: ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”(Group Relative Policy Optimization) ë“±

### 2.2 ëª¨ë¸ í™•ì¥  
- **AutoModelForCausalLMWithValueHead**, **AutoModelForSeq2SeqLMWithValueHead**: ë³´ìƒ í•¨ìˆ˜ìš© value headê°€ ê²°í•©ëœ ëª¨ë¸ í´ë˜ìŠ¤ ì œê³µ  
- **TRLX**: CarperAIê°€ ê°œë°œí•œ TRL í¬í¬ë¡œ ëŒ€ê·œëª¨(ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„°) ëª¨ë¸ ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ í•™ìŠµ ì§€ì›

### 2.3 ë©€í‹°ëª¨ë‹¬ ë° VLM ì •ë ¬  
- ìµœê·¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM) ì •ë ¬ìš© RLHF ê¸°ëŠ¥ ì¶”ê°€  
- ë¦¬ì›Œë“œ ëª¨ë¸ê³¼ RL ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•´ VLMì˜ "hallucination(í—ˆìœ„ ìƒì„±)" í˜„ìƒ ê°ì†Œ

## 3. ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •  
```bash
# ìµœì‹  ë²„ì „ ì„¤ì¹˜
pip install trl
# ë˜ëŠ” GitHub ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜ (ê°œë°œìš©)
git clone https://github.com/huggingface/trl.git
cd trl
pip install -e .
```
ì„¤ì¹˜ ì‹œ ğŸ¤— Transformers, torch, accelerate ë“±ì´ ìë™ìœ¼ë¡œ ì¢…ì†ì„±ì— í¬í•¨ëœë‹¤.

## 4. ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°  

### 4.1 PPOTrainer ì˜ˆì œ  
```python
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model, respond_to_batch
import torch

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
model_ref = create_reference_model(model)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# PPO ì„¤ì •
ppo_config = PPOConfig(batch_size=1, forward_batch_size=1)

# ì§ˆì˜(query) í…ì„œ ìƒì„±
query_txt = "This morning I went to the "
query_tensor = tokenizer.encode(query_txt, return_tensors="pt")

# ëª¨ë¸ ì‘ë‹µ ìƒì„±
response_tensor = respond_to_batch(model_ref, query_tensor)

# ë³´ìƒê°’ ì •ì˜ (ì˜ˆ: ë‹¤ë¥¸ ëª¨ë¸ ì¶œë ¥ ë˜ëŠ” ì‚¬ëŒ í”¼ë“œë°±)
reward = [torch.tensor(1.0)]

# PPO í•™ìŠµ ì§„í–‰
ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)
train_stats = ppo_trainer.step([query_tensor], [response_tensor], reward)
```
ì´ì²˜ëŸ¼ **(query, response, reward)** ì‚¼ì¤‘í•­ë§Œìœ¼ë¡œ í•œ ë²ˆì˜ í•™ìŠµ ìŠ¤í…ì´ ì‹¤í–‰ëœë‹¤.

### 4.2 SFTTrainerì™€ RewardTrainer  
```python
from trl import SFTTrainer, RewardTrainer

# SFTTrainer: ì§€ë„í•™ìŠµ ë¯¸ì„¸ì¡°ì •
sft_trainer = SFTTrainer(model=model, args=args, train_dataset=train_ds, tokenizer=tokenizer)
sft_trainer.train()

# RewardTrainer: ë³´ìƒëª¨ë¸ í•™ìŠµ
reward_trainer = RewardTrainer(model=reward_model, args=args, train_dataset=rm_ds, tokenizer=tokenizer)
reward_trainer.train()
```
ê° TrainerëŠ” TrainingArguments ë˜ëŠ” TRL ì „ìš© ì„¤ì •ì„ í†µí•´ ë°°ì¹˜ í¬ê¸°, í•™ìŠµë¥ , ë¡œê¹…, ë¶„ì‚° í›ˆë ¨ ë“± ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤.

## 5. ì„±ëŠ¥ ìµœì í™” ë° í™•ì¥  

### 5.1 vLLM í†µí•©  
ì˜¨ë¼ì¸ RL ê¸°ë²•(GRPO, DPO) ì‚¬ìš© ì‹œ **use_vllm** í”Œë˜ê·¸ë¥¼ í†µí•´ vLLM ì—”ì§„ì„ ìë™ í˜¸ì¶œí•˜ì—¬ ëŒ€ê·œëª¨ ìƒ˜í”Œë§ì„ ê°€ì†í™”í•  ìˆ˜ ìˆë‹¤.

### 5.2 ë¶„ì‚° í•™ìŠµ ë° í•˜ë“œì›¨ì–´ ìµœì í™”  
- DeepSpeed ZeRO, FSDP, Liger Kernel í†µí•© ê°€ì´ë“œ ì œê³µ  
- PEFT ê¸°ë°˜ LoRA/QLoRA ì–´ëŒ‘í„° ê²°í•©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™” ê°€ëŠ¥  

## 6. ì‹¤ì œ ì ìš© ì‚¬ë¡€  

- **ì˜í™” ë¦¬ë·° ìƒì„±**: GPT2ë¥¼ í† í°ë³„ ê°ì • ë¶„ë¥˜ ë³´ìƒ ëª¨ë¸ê³¼ ê²°í•©í•˜ì—¬ ê¸ì • ë¦¬ë·° ìƒì„±  
- **ì±—ë´‡ ì •ë ¬**: ChatGPT ìœ ì‚¬ ëª¨ë¸ì— RLHF ì ìš©í•˜ì—¬ ì‚¬ìš©ì ë§Œì¡±ë„ ê¸°ë°˜ ì‘ë‹µ ê°œì„   
- **VLM ìº¡ì…˜ ì •ë ¬**: ì´ë¯¸ì§€ ìº¡ì…˜ VLMì—ì„œ í—ˆìœ„ ì •ë³´ ê°ì†Œ ëª©ì ìœ¼ë¡œ RLHF ì ìš©  

TRL ê¸°ë°˜ í”„ë¡œì íŠ¸ëŠ” ê¸°ì—… ë° ì—°êµ¬ì—ì„œ RLHF ì›Œí¬í”Œë¡œìš°ë¥¼ **"ì œë¡œë¶€í„° ì§ì ‘ êµ¬í˜„í•˜ì§€ ì•Šê³ ë„"** ì¦‰ì‹œ ì‹¤í—˜ ê°€ëŠ¥í•œ í™˜ê²½ìœ¼ë¡œ ì œê³µí•œë‹¤.

## 7. ì»¤ë®¤ë‹ˆí‹° ë° ìƒíƒœê³„  

- **GitHub**: huggingface/trl (stars 6,400+), ì£¼ê¸°ì  ë¦´ë¦¬ìŠ¤ ì œê³µ
- **Hugging Face Hub**: TRL ëª¨ë¸Â·ë°ì´í„°ì…‹Â·ë°ëª¨ ì¡°ì§ ìš´ì˜  
- **ë¸”ë¡œê·¸**: "NO GPU left behind", "Liger GRPO meets TRL" ë“± ìµœì‹  ìµœì í™” ë¡œë“œë§µ ê³µê°œ

## 8. ê²°ë¡   
TRLì€ **LLMì˜ ì¸ê°„ ì¤‘ì‹¬ ì •ë ¬(HRLHF)**ì„ ìœ„í•œ ì „ë¬¸ í”Œë«í¼ìœ¼ë¡œ, SFTâ†’Reward Modelingâ†’PPO ë‹¨ê³„ë³„ í•™ìŠµì„ ì™„ì „ ìë™í™”í•œë‹¤. ğŸ¤— Transformersì™€ ê¸´ë°€íˆ í†µí•©ë˜ì–´ **ë§¤ìš° ì§§ì€ ì½”ë“œ**ë¡œ RL ê¸°ë°˜ ë¯¸ì„¸ì¡°ì •ì„ ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë©°, vLLMÂ·DeepSpeedÂ·PEFT ë“± ìµœì²¨ë‹¨ ìƒíƒœê³„ì™€ì˜ ì—°ë™ì„ í†µí•´ **íš¨ìœ¨ì„±ê³¼ í™•ì¥ì„±**ì„ ë™ì‹œì— í™•ë³´í•œë‹¤. 

- **ì¶”ì²œ ëŒ€ìƒ**: RLHF ì›Œí¬í”Œë¡œìš°ë¥¼ ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ë ¤ëŠ” ì—°êµ¬ìÂ·ì—”ì§€ë‹ˆì–´  
- **ê³µì‹ ë¬¸ì„œ**: https://huggingface.co/docs/trl/index  
- **GitHub**: https://github.com/huggingface/trl

**í•µì‹¬ í‚¤ì›Œë“œ**: RLHF, PPO, DPO, GRPO, SFT, Reward Modeling, ê°•í™”í•™ìŠµ, ì¸ê°„ í”¼ë“œë°±, ëª¨ë¸ ì •ë ¬